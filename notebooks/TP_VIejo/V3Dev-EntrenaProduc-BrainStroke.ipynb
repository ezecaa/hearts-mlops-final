{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2512debf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución antes de SMOTE:\n",
      "stroke\n",
      "0    0.951309\n",
      "1    0.048691\n",
      "Name: proportion, dtype: float64\n",
      "Forma después de SMOTE: (7776, 13)\n",
      "Distribución después de SMOTE:\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "Usando CPU\n",
      "\n",
      "--- Iniciando Entrenamiento ---\n",
      "Epoch [1/100], Loss: 0.5196\n",
      "Epoch [2/100], Loss: 0.4452\n",
      "Epoch [3/100], Loss: 0.4235\n",
      "Epoch [4/100], Loss: 0.4137\n",
      "Epoch [5/100], Loss: 0.3976\n",
      "Epoch [6/100], Loss: 0.3886\n",
      "Epoch [7/100], Loss: 0.3824\n",
      "Epoch [8/100], Loss: 0.3752\n",
      "Epoch [9/100], Loss: 0.3682\n",
      "Epoch [10/100], Loss: 0.3595\n",
      "Epoch [10/100] - Evaluación Test: Loss: 0.4430, Accuracy: 75.44%\n",
      "Epoch [11/100], Loss: 0.3482\n",
      "Epoch [12/100], Loss: 0.3410\n",
      "Epoch [13/100], Loss: 0.3395\n",
      "Epoch [14/100], Loss: 0.3348\n",
      "Epoch [15/100], Loss: 0.3279\n",
      "Epoch [16/100], Loss: 0.3232\n",
      "Epoch [17/100], Loss: 0.3180\n",
      "Epoch [18/100], Loss: 0.3070\n",
      "Epoch [19/100], Loss: 0.3054\n",
      "Epoch [20/100], Loss: 0.3088\n",
      "Epoch [20/100] - Evaluación Test: Loss: 0.4054, Accuracy: 78.47%\n",
      "Epoch [21/100], Loss: 0.3004\n",
      "Epoch [22/100], Loss: 0.3036\n",
      "Epoch [23/100], Loss: 0.2935\n",
      "Epoch [24/100], Loss: 0.2935\n",
      "Epoch [25/100], Loss: 0.2898\n",
      "Epoch [26/100], Loss: 0.2884\n",
      "Epoch [27/100], Loss: 0.2784\n",
      "Epoch [28/100], Loss: 0.2839\n",
      "Epoch [29/100], Loss: 0.2761\n",
      "Epoch [30/100], Loss: 0.2722\n",
      "Epoch [30/100] - Evaluación Test: Loss: 0.3951, Accuracy: 81.41%\n",
      "Epoch [31/100], Loss: 0.2599\n",
      "Epoch [32/100], Loss: 0.2675\n",
      "Epoch [33/100], Loss: 0.2659\n",
      "Epoch [34/100], Loss: 0.2627\n",
      "Epoch [35/100], Loss: 0.2730\n",
      "Epoch [36/100], Loss: 0.2588\n",
      "Epoch [37/100], Loss: 0.2674\n",
      "Epoch [38/100], Loss: 0.2591\n",
      "Epoch [39/100], Loss: 0.2526\n",
      "Epoch [40/100], Loss: 0.2476\n",
      "Epoch [40/100] - Evaluación Test: Loss: 0.4195, Accuracy: 81.41%\n",
      "Epoch [41/100], Loss: 0.2579\n",
      "Epoch [42/100], Loss: 0.2474\n",
      "Epoch [43/100], Loss: 0.2444\n",
      "Epoch [44/100], Loss: 0.2474\n",
      "Epoch [45/100], Loss: 0.2490\n",
      "Epoch [46/100], Loss: 0.2466\n",
      "Epoch [47/100], Loss: 0.2477\n",
      "Epoch [48/100], Loss: 0.2393\n",
      "Epoch [49/100], Loss: 0.2360\n",
      "Epoch [50/100], Loss: 0.2391\n",
      "Epoch [50/100] - Evaluación Test: Loss: 0.3927, Accuracy: 83.66%\n",
      "Epoch [51/100], Loss: 0.2439\n",
      "Epoch [52/100], Loss: 0.2391\n",
      "Epoch [53/100], Loss: 0.2334\n",
      "Epoch [54/100], Loss: 0.2310\n",
      "Epoch [55/100], Loss: 0.2304\n",
      "Epoch [56/100], Loss: 0.2323\n",
      "Epoch [57/100], Loss: 0.2335\n",
      "Epoch [58/100], Loss: 0.2302\n",
      "Epoch [59/100], Loss: 0.2291\n",
      "Epoch [60/100], Loss: 0.2235\n",
      "Epoch [60/100] - Evaluación Test: Loss: 0.4150, Accuracy: 84.34%\n",
      "Epoch [61/100], Loss: 0.2238\n",
      "Epoch [62/100], Loss: 0.2190\n",
      "Epoch [63/100], Loss: 0.2329\n",
      "Epoch [64/100], Loss: 0.2298\n",
      "Epoch [65/100], Loss: 0.2209\n",
      "Epoch [66/100], Loss: 0.2261\n",
      "Epoch [67/100], Loss: 0.2225\n",
      "Epoch [68/100], Loss: 0.2250\n",
      "Epoch [69/100], Loss: 0.2290\n",
      "Epoch [70/100], Loss: 0.2270\n",
      "Epoch [70/100] - Evaluación Test: Loss: 0.4171, Accuracy: 84.25%\n",
      "Epoch [71/100], Loss: 0.2147\n",
      "Epoch [72/100], Loss: 0.2132\n",
      "Epoch [73/100], Loss: 0.2175\n",
      "Epoch [74/100], Loss: 0.2127\n",
      "Epoch [75/100], Loss: 0.2200\n",
      "Epoch [76/100], Loss: 0.2138\n",
      "Epoch [77/100], Loss: 0.2145\n",
      "Epoch [78/100], Loss: 0.2130\n",
      "Epoch [79/100], Loss: 0.2089\n",
      "Epoch [80/100], Loss: 0.2052\n",
      "Epoch [80/100] - Evaluación Test: Loss: 0.4320, Accuracy: 84.44%\n",
      "Epoch [81/100], Loss: 0.2139\n",
      "Epoch [82/100], Loss: 0.2104\n",
      "Epoch [83/100], Loss: 0.2101\n",
      "Epoch [84/100], Loss: 0.2154\n",
      "Epoch [85/100], Loss: 0.2098\n",
      "Epoch [86/100], Loss: 0.2118\n",
      "Epoch [87/100], Loss: 0.2033\n",
      "Epoch [88/100], Loss: 0.2040\n",
      "Epoch [89/100], Loss: 0.2128\n",
      "Epoch [90/100], Loss: 0.2045\n",
      "Epoch [90/100] - Evaluación Test: Loss: 0.4234, Accuracy: 84.44%\n",
      "Epoch [91/100], Loss: 0.1990\n",
      "Epoch [92/100], Loss: 0.2056\n",
      "Epoch [93/100], Loss: 0.2059\n",
      "Epoch [94/100], Loss: 0.2098\n",
      "Epoch [95/100], Loss: 0.2029\n",
      "Epoch [96/100], Loss: 0.1962\n",
      "Epoch [97/100], Loss: 0.1989\n",
      "Epoch [98/100], Loss: 0.2077\n",
      "Epoch [99/100], Loss: 0.1957\n",
      "Epoch [100/100], Loss: 0.2043\n",
      "Epoch [100/100] - Evaluación Test: Loss: 0.4003, Accuracy: 85.13%\n",
      "\n",
      "--- Entrenamiento Finalizado ---\n",
      "\n",
      "--- Reporte de Clasificación Final (Test Set) ---\n",
      "[[852 120]\n",
      " [ 32  18]]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "No Stroke (0)       0.96      0.88      0.92       972\n",
      "   Stroke (1)       0.13      0.36      0.19        50\n",
      "\n",
      "     accuracy                           0.85      1022\n",
      "    macro avg       0.55      0.62      0.55      1022\n",
      " weighted avg       0.92      0.85      0.88      1022\n",
      "\n",
      "Accuracy Final en Test: 85.13%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE # Para balancear el dataset\n",
    "\n",
    "# --- Código de Preprocesamiento de Datos (Proporcionado por el usuario) ---\n",
    "\n",
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Realiza la limpieza inicial del conjunto de datos\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    # Eliminar columna 'id' si existe y no es útil como característica\n",
    "    if 'id' in df_clean.columns:\n",
    "        df_clean = df_clean.drop(columns=['id'])\n",
    "\n",
    "    # Manejo de valores faltantes en BMI\n",
    "    # Asegurarse de que 'Other' en gender no cause problemas si es una categoría rara\n",
    "    df_clean = df_clean[df_clean['gender'] != 'Other'] # O imputar si se prefiere\n",
    "\n",
    "    # Imputar BMI ANTES de eliminar outliers que puedan afectarlo\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df_clean['bmi'] = imputer.fit_transform(df_clean[['bmi']])\n",
    "\n",
    "    # Eliminar filas donde 'smoking_status' es 'Unknown' podría ser una opción,\n",
    "    # o tratarlo como una categoría separada si LabelEncoder lo maneja.\n",
    "    # Por ahora, LabelEncoder lo tratará como una categoría más.\n",
    "\n",
    "    # Eliminar valores atípicos extremos en glucose_level (opcional, considerar impacto)\n",
    "    # q_low = df_clean['avg_glucose_level'].quantile(0.01) # Considerar umbrales menos extremos\n",
    "    # q_high = df_clean['avg_glucose_level'].quantile(0.99)\n",
    "    # df_clean = df_clean[\n",
    "    #     (df_clean['avg_glucose_level'] >= q_low) &\n",
    "    #     (df_clean['avg_glucose_level'] <= q_high)\n",
    "    # ]\n",
    "    # Considerar también outliers en 'age' y 'bmi' si es necesario\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Realiza la ingeniería de características\n",
    "    \"\"\"\n",
    "    df_engineered = df.copy()\n",
    "\n",
    "    # Crear categorías de BMI\n",
    "    df_engineered['bmi_category'] = pd.cut(\n",
    "        df_engineered['bmi'],\n",
    "        bins=[0, 18.5, 24.9, 29.9, np.inf],\n",
    "        labels=['Bajo peso', 'Normal', 'Sobrepeso', 'Obeso'],\n",
    "        right=False # Asegura que 18.5 cae en 'Normal', etc.\n",
    "    )\n",
    "\n",
    "    # Crear categorías de edad\n",
    "    df_engineered['age_group'] = pd.cut(\n",
    "        df_engineered['age'],\n",
    "        bins=[0, 18, 35, 50, 65, np.inf],\n",
    "        labels=['<18', '18-35', '36-50', '51-65', '>65'],\n",
    "        right=False\n",
    "    )\n",
    "\n",
    "    # Crear categorías de glucosa\n",
    "    df_engineered['glucose_category'] = pd.cut(\n",
    "        df_engineered['avg_glucose_level'],\n",
    "        bins=[0, 70, 100, 125, np.inf],\n",
    "        labels=['Bajo', 'Normal', 'Pre-diabetes', 'Diabetes'],\n",
    "        right=False\n",
    "    )\n",
    "\n",
    "    # Convertir las nuevas columnas categóricas a tipo 'category' puede ser útil\n",
    "    for col in ['bmi_category', 'age_group', 'glucose_category']:\n",
    "         if col in df_engineered.columns: # Check if column exists after potential filtering\n",
    "            df_engineered[col] = df_engineered[col].astype('category')\n",
    "\n",
    "\n",
    "    return df_engineered\n",
    "\n",
    "def encode_variables(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Codifica las variables categóricas usando Label Encoding.\n",
    "    PRECAUCIÓN: LabelEncoder asigna números arbitrarios (0, 1, 2...).\n",
    "    Para redes neuronales, One-Hot Encoding suele ser preferible para\n",
    "    variables nominales (sin orden inherente) para evitar que el modelo\n",
    "    interprete un orden inexistente. Para simplificar, usamos LabelEncoder aquí,\n",
    "    pero considera pd.get_dummies() para un enfoque más robusto.\n",
    "    \"\"\"\n",
    "    df_encoded = df.copy()\n",
    "\n",
    "    # Identificar columnas categóricas (object o category dtype)\n",
    "    categorical_columns = df_encoded.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    encoders = {} # Guardar encoders por si se necesitan después\n",
    "    for column in categorical_columns:\n",
    "        # Asegurarse de que no haya NaNs antes de codificar\n",
    "        if df_encoded[column].isnull().any():\n",
    "           # Opción 1: Imputar con una categoría específica como 'Desconocido'\n",
    "           df_encoded[column] = df_encoded[column].cat.add_categories('Desconocido').fillna('Desconocido')\n",
    "           # Opción 2: Imputar con la moda (si aplica)\n",
    "           # mode_val = df_encoded[column].mode()[0]\n",
    "           # df_encoded[column] = df_encoded[column].fillna(mode_val)\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        df_encoded[column] = le.fit_transform(df_encoded[column])\n",
    "        encoders[column] = le # Guardar el encoder\n",
    "\n",
    "    # Eliminar las columnas numéricas originales que fueron categorizadas si se desea\n",
    "    # df_encoded = df_encoded.drop(columns=['age', 'bmi', 'avg_glucose_level'], errors='ignore')\n",
    "\n",
    "    return df_encoded, encoders # Devolver encoders puede ser útil\n",
    "\n",
    "# Función para escalar y dividir datos\n",
    "def prepare_for_modeling(df: pd.DataFrame,\n",
    "                         target: str = 'stroke',\n",
    "                         test_size: float = 0.2,\n",
    "                         random_state: int = 42) -> tuple:\n",
    "    \"\"\"\n",
    "    Prepara los datos para el modelado: separa X/y, divide train/test, y escala X.\n",
    "    \"\"\"\n",
    "    # Separar características y objetivo\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "\n",
    "    # División train-test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y # Stratify es importante en clasificación desbalanceada\n",
    "    )\n",
    "\n",
    "    # Escalado de características numéricas\n",
    "    # Identificar columnas numéricas para escalar (excluir las ya codificadas si son el resultado final)\n",
    "    # Si usamos LabelEncoder en todo, todas las X serán numéricas en este punto.\n",
    "    numeric_cols = X_train.select_dtypes(include=np.number).columns # Asumiendo que todas son numéricas tras encode\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    # Ajustar y transformar en el conjunto de entrenamiento\n",
    "    X_train_scaled = scaler.fit_transform(X_train[numeric_cols])\n",
    "    # Solo transformar en el conjunto de prueba (usando el ajuste del entrenamiento)\n",
    "    X_test_scaled = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "    # Convertir de nuevo a DataFrame para mantener nombres de columnas (opcional pero útil)\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, index=X_train.index, columns=numeric_cols)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, index=X_test.index, columns=numeric_cols)\n",
    "\n",
    "    # Asegurarse de que y_train, y_test sean Series de Pandas para el paso de SMOTE\n",
    "    if not isinstance(y_train, pd.Series): y_train = pd.Series(y_train, index=X_train.index)\n",
    "    if not isinstance(y_test, pd.Series): y_test = pd.Series(y_test, index=X_test.index)\n",
    "\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
    "\n",
    "def balance_dataset(X_train, y_train, random_state=42):\n",
    "    \"\"\"\n",
    "    Aplica SMOTE para balancear el conjunto de datos de entrenamiento.\n",
    "    \"\"\"\n",
    "    print(f\"Distribución antes de SMOTE:\\n{y_train.value_counts(normalize=True)}\")\n",
    "    # Manejo del desbalance de clases con SMOTE\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    # SMOTE espera arrays de numpy generalmente\n",
    "    X_train_np = X_train.to_numpy() if isinstance(X_train, pd.DataFrame) else X_train\n",
    "    y_train_np = y_train.to_numpy() if isinstance(y_train, pd.Series) else y_train\n",
    "\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_np, y_train_np)\n",
    "    print(f\"Forma después de SMOTE: {X_train_balanced.shape}\")\n",
    "    print(f\"Distribución después de SMOTE:\\n{pd.Series(y_train_balanced).value_counts(normalize=True)}\")\n",
    "    return X_train_balanced, y_train_balanced\n",
    "\n",
    "\n",
    "# --- Definición de la Red Neuronal (Proporcionada por el usuario) ---\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes=[128, 64, 32]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "\n",
    "        # Capas ocultas con normalización por lotes y activación mejorada\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size), # BatchNorm ayuda a estabilizar\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3) # Dropout para regularización\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Capa de salida\n",
    "        layers.append(nn.Linear(prev_size, 1)) # Salida única para clasificación binaria\n",
    "        # No añadimos Sigmoid aquí, usaremos BCEWithLogitsLoss que es más estable\n",
    "        # layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "# --- Función para preparar datos para PyTorch ---\n",
    "def prepare_data(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Prepara los datos (NumPy arrays) para PyTorch y los mueve al dispositivo.\n",
    "    \"\"\"\n",
    "    # Configurar dispositivo para PyTorch\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Usando Apple Metal (MPS)\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Usando CUDA GPU\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Usando CPU\")\n",
    "\n",
    "    # Convertir DataFrames/Series a arreglos numpy si aún no lo son\n",
    "    if isinstance(X_train, pd.DataFrame): X_train = X_train.to_numpy()\n",
    "    if isinstance(X_test, pd.DataFrame): X_test = X_test.to_numpy()\n",
    "    if isinstance(y_train, pd.Series): y_train = y_train.to_numpy()\n",
    "    if isinstance(y_test, pd.Series): y_test = y_test.to_numpy()\n",
    "\n",
    "    # Asegurar que X_train y X_test sean bidimensionales (ya deberían serlo tras StandardScaler)\n",
    "    if len(X_train.shape) == 1: X_train = X_train.reshape(-1, 1)\n",
    "    if len(X_test.shape) == 1: X_test = X_test.reshape(-1, 1)\n",
    "\n",
    "    # Convertir a Tensores de PyTorch (usar FloatTensor para X, podría ser LongTensor para y si la loss lo requiere, pero BCEWithLogitsLoss prefiere Float)\n",
    "    X_train = torch.FloatTensor(X_train).to(device)\n",
    "    X_test = torch.FloatTensor(X_test).to(device)\n",
    "    y_train = torch.FloatTensor(y_train).reshape(-1, 1).to(device) # BCEWithLogitsLoss espera y como Float\n",
    "    y_test = torch.FloatTensor(y_test).reshape(-1, 1).to(device)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, device\n",
    "\n",
    "\n",
    "# --- Flujo Principal ---\n",
    "\n",
    "# 1. Carga de datos\n",
    "DATA_PATH = \"data/healthcare-dataset-stroke-data.csv\" # Asegúrate que la ruta sea correcta\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No se encontró el archivo en {DATA_PATH}\")\n",
    "    exit() # Salir si no se encuentra el archivo\n",
    "\n",
    "\n",
    "# 2. Preprocesamiento\n",
    "df_clean = clean_data(df)\n",
    "df_engineered = engineer_features(df_clean)\n",
    "df_encoded, _ = encode_variables(df_engineered) # Guardamos los encoders por si acaso\n",
    "\n",
    "# 3. Preparación para Modelado (Divide y Escala)\n",
    "X_train_scaled, X_test_scaled, y_train, y_test, scaler = prepare_for_modeling(df_encoded)\n",
    "\n",
    "# 4. Balanceo del Dataset de Entrenamiento (SMOTE)\n",
    "#    Asegúrate que X_train_scaled es NumPy array o compatible con SMOTE\n",
    "X_train_balanced, y_train_balanced = balance_dataset(X_train_scaled, y_train)\n",
    "\n",
    "# 5. Preparación para PyTorch (Convertir a Tensores y mover a Dispositivo)\n",
    "X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor, device = prepare_data(\n",
    "    X_train_balanced, X_test_scaled.to_numpy(), y_train_balanced, y_test.to_numpy() # Asegurar que X_test es numpy\n",
    ")\n",
    "\n",
    "# --- Configuración del Entrenamiento ---\n",
    "\n",
    "# Hiperparámetros\n",
    "INPUT_SIZE = X_train_tensor.shape[1] # Número de características tras preprocesamiento\n",
    "HIDDEN_SIZES = [128, 64, 32] # Arquitectura definida en la clase\n",
    "OUTPUT_SIZE = 1 # Salida única para clasificación binaria\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 100 # Número de veces que se itera sobre todo el dataset\n",
    "BATCH_SIZE = 64 # Tamaño de los lotes de datos para entrenar\n",
    "\n",
    "# Crear DataLoaders para manejar los lotes eficientemente\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False) # No barajar en test\n",
    "\n",
    "# Instanciar el modelo y moverlo al dispositivo\n",
    "model = FeedForwardNN(input_size=INPUT_SIZE, hidden_sizes=HIDDEN_SIZES).to(device)\n",
    "\n",
    "# Función de Pérdida y Optimizador\n",
    "# Usar BCEWithLogitsLoss es numéricamente más estable que Sigmoid + BCELoss\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# --- Bucle de Entrenamiento ---\n",
    "print(\"\\n--- Iniciando Entrenamiento ---\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train() # Poner el modelo en modo entrenamiento (activa Dropout, BatchNorm en modo train)\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Los DataLoaders ya deberían tener los tensores en el dispositivo correcto\n",
    "        # si se crearon a partir de tensores ya movidos. Si no:\n",
    "        # inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels) # Calcular la pérdida\n",
    "\n",
    "        # Backward pass y optimización\n",
    "        optimizer.zero_grad() # Limpiar gradientes anteriores\n",
    "        loss.backward() # Calcular gradientes\n",
    "        optimizer.step() # Actualizar pesos\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Imprimir pérdida promedio de la época\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # --- Evaluación Opcional por Época (en el conjunto de test) ---\n",
    "    if (epoch + 1) % 10 == 0: # Evaluar cada 10 épocas, por ejemplo\n",
    "        model.eval() # Poner el modelo en modo evaluación (desactiva Dropout, BatchNorm usa estadísticas acumuladas)\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad(): # Desactivar cálculo de gradientes para evaluación\n",
    "            for inputs, labels in test_loader:\n",
    "                # inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                # Convertir logits a probabilidades (aplicando Sigmoid) y luego a predicciones (0 o 1)\n",
    "                predicted = torch.sigmoid(outputs) > 0.5\n",
    "                # predicted = (outputs > 0).float() # Alternativa si se usa BCEWithLogitsLoss (logits > 0 equivale a sigmoid > 0.5)\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                # Guardar para reporte de clasificación\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}] - Evaluación Test: Loss: {avg_test_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "        # print(classification_report(all_labels, all_preds, target_names=['No Stroke', 'Stroke']))\n",
    "\n",
    "\n",
    "print(\"\\n--- Entrenamiento Finalizado ---\")\n",
    "\n",
    "# --- Evaluación Final ---\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        # Convertir logits a predicciones\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"\\n--- Reporte de Clasificación Final (Test Set) ---\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n",
    "print(classification_report(all_labels, all_preds, target_names=['No Stroke (0)', 'Stroke (1)']))\n",
    "final_accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Accuracy Final en Test: {final_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Puedes guardar el modelo entrenado si lo deseas\n",
    "# torch.save(model.state_dict(), 'stroke_ffnn_model.pth')\n",
    "# print(\"Modelo guardado en stroke_ffnn_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa284fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
